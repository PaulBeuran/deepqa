{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==1.11 in /home/pbeuran/.local/lib/python3.8/site-packages (1.11.0)\n",
      "Requirement already satisfied: transformers in /home/pbeuran/.local/lib/python3.8/site-packages (4.19.2)\n",
      "Collecting stanza\n",
      "  Downloading stanza-1.4.0-py3-none-any.whl (574 kB)\n",
      "\u001b[K     |████████████████████████████████| 574 kB 22.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /home/pbeuran/.local/lib/python3.8/site-packages (from torch==1.11) (4.2.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/pbeuran/.local/lib/python3.8/site-packages (from transformers) (2022.4.24)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/pbeuran/.local/lib/python3.8/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/pbeuran/.local/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/pbeuran/.local/lib/python3.8/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/pbeuran/.local/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: filelock in /home/pbeuran/.local/lib/python3.8/site-packages (from transformers) (3.7.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/pbeuran/.local/lib/python3.8/site-packages (from transformers) (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/pbeuran/.local/lib/python3.8/site-packages (from transformers) (1.22.1)\n",
      "Requirement already satisfied: requests in /home/pbeuran/.local/lib/python3.8/site-packages (from transformers) (2.27.1)\n",
      "Collecting emoji\n",
      "  Downloading emoji-2.0.0.tar.gz (197 kB)\n",
      "\u001b[K     |████████████████████████████████| 197 kB 23.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf in /home/pbeuran/.local/lib/python3.8/site-packages (from stanza) (3.20.1)\n",
      "Requirement already satisfied: six in /home/pbeuran/.local/lib/python3.8/site-packages (from stanza) (1.16.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/pbeuran/.local/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/pbeuran/.local/lib/python3.8/site-packages (from requests->transformers) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /home/pbeuran/.local/lib/python3.8/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /home/pbeuran/.local/lib/python3.8/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/pbeuran/.local/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "Building wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for emoji: filename=emoji-2.0.0-py3-none-any.whl size=193002 sha256=f2ba39c5bfc065aae279bed1af2dbc61228ec33cdc462937c87bff48a815fdbb\n",
      "  Stored in directory: /home/pbeuran/.cache/pip/wheels/23/a5/a8/e74bad1ceced228b6ae94dcbacc5c67df6486fd1620714e7d1\n",
      "Successfully built emoji\n",
      "Installing collected packages: emoji, stanza\n",
      "Successfully installed emoji-2.0.0 stanza-1.4.0\n"
     ]
    }
   ],
   "source": [
    "#####################################################################\n",
    "# Mandatory : Run this cell and restart the notebook kernel right after\n",
    "#####################################################################\n",
    "!pip install torch==1.11 transformers stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/content'\n",
      "/home/pbeuran/personal_repositories/deepqa/notebooks\n"
     ]
    }
   ],
   "source": [
    "%cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#####################################################################\n",
    "# Only use on Googgle Colab, uncomment if necessary\n",
    "#####################################################################\n",
    "# Clone the repo content into\n",
    "cd /content\n",
    "rm -rf deepqa\n",
    "git clone -b model https://github.com/PaulBeuran/deepqa.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#####################################################################\n",
    "# Only use on Googgle Colab, uncomment if necessary\n",
    "#####################################################################\n",
    "# Clone the repo content into\n",
    "cd /content\n",
    "rm -rf deepqa\n",
    "git clone -b model https://github.com/PaulBeuran/deepqa.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'deepqa/notebooks/'\n",
      "/home/pbeuran/personal_repositories/deepqa/notebooks\n"
     ]
    }
   ],
   "source": [
    "#####################################################################\n",
    "# Only use on Googgle Colab, uncomment if necessary\n",
    "#####################################################################\n",
    "%cd deepqa/notebooks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  822M  100  822M    0     0  4124k      0  0:03:24  0:03:24 --:--:-- 4146k\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  glove.6B.zip\n",
      "  inflating: word_encoders_configs/glove.6B.50d.txt  \n",
      "  inflating: word_encoders_configs/glove.6B.100d.txt  \n",
      "  inflating: word_encoders_configs/glove.6B.200d.txt  \n",
      "  inflating: word_encoders_configs/glove.6B.300d.txt  \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ..\n",
    "curl -O https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
    "unzip -o glove.6B.zip -d word_encoders_configs\n",
    "rm glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-30 00:36:05.538503: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-30 00:36:05.538577: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Dowload the SQuAD1.1 data\n",
    "curl -O https://data.deepai.org/squad1.1.zip\n",
    "unzip -o squad1.1.zip -d data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-05 11:49:00.872813: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-08-05 11:49:00.872914: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "deepqa_lib_path = str(pathlib.Path(os.getcwd()).parent.parent.absolute())\n",
    "sys.path.insert(0, deepqa_lib_path)\n",
    "\n",
    "from deepqa import preprocessing, tokenizer, model, wrapper, loss, metrics, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read train and dev data\n",
    "with open(\"data/train-v1.1.json\", \"rb\") as j:\n",
    "    train_data = json.load(j)[\"data\"]\n",
    "with open(\"data/dev-v1.1.json\", \"rb\") as j:\n",
    "    dev_data = json.load(j)[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tcqa = preprocessing.tabularize_squad11_data(train_data)\n",
    "dev_tcqa = preprocessing.tabularize_squad11_data(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-05 11:49:04.939764: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-08-05 11:49:04.939860: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-08-05 11:49:04.939898: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-6LCD53E): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from spacy.lang.en import English\n",
    "import pickle\n",
    "from deepqa.utils import char_ranges_to_token_ranges\n",
    "\n",
    "class GLoVETokenizer():\n",
    "\n",
    "    def __init__(self, corpus=\"6B\", tokenize_char=False):\n",
    "\n",
    "        self.word_tokenizer = English().tokenizer\n",
    "        with open(f\"../word_encoders_configs/glove.{corpus}.50d.txt\", \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "        lines = [line.replace(\"\\n\", \"\").split(\" \") for line in lines]\n",
    "        word_vocab = [line[0] for line in lines]\n",
    "        self.word_vocab_id_mapping = {**dict(zip(word_vocab, range(len(word_vocab)))),\n",
    "                                      **{\"\": len(word_vocab) + 1}}\n",
    "        self.word_vocab_len = len(self.word_vocab_id_mapping)\n",
    "        self.tokenize_char = tokenize_char\n",
    "        if tokenize_char:\n",
    "            char_token_vocab_path = \"../tokenizers_configs\"\n",
    "            char_vocab = pickle.load(open(f\"{char_token_vocab_path}/bert-base-uncased.ctk\", \"rb\"))\n",
    "            self.char_vocab_len = len(char_vocab)\n",
    "            self.char_vocab_id_mapping = dict(zip(char_vocab, range(len(char_vocab))))\n",
    "\n",
    "\n",
    "    def __call__(self, texts, tokens_max_length, token_max_length, **kwargs):\n",
    "\n",
    "        tokenize_result = dict()\n",
    "\n",
    "        texts = [re.sub(r\" {2,}\", \"\", text) for text in texts]\n",
    "\n",
    "        inputs_tokens = [list(doc) for doc in self.word_tokenizer.pipe(texts)]\n",
    "\n",
    "        inputs_truncated = [len(input_tokens) > tokens_max_length\n",
    "                            for input_tokens in inputs_tokens]\n",
    "        inputs_tokens = [input_tokens[:min(len(input_tokens), tokens_max_length)] \n",
    "                         for input_tokens in inputs_tokens]\n",
    "        inputs_low_pad_tokens = [[str(input_token).lower() \n",
    "                                  for input_token in input_tokens] +\n",
    "                                 (tokens_max_length - len(input_tokens)) * [\"\"]\n",
    "                                for input_tokens in inputs_tokens]\n",
    "        tokenize_result[\"truncated\"] = torch.tensor(inputs_truncated)\n",
    "\n",
    "        inputs_ids = [[self.word_vocab_id_mapping.get(input_low_pad_token, \n",
    "                                                      self.word_vocab_len)\n",
    "                       for input_low_pad_token in input_low_pad_tokens]\n",
    "                      for input_low_pad_tokens in inputs_low_pad_tokens]\n",
    "        tokenize_result[\"input_ids\"] = torch.tensor(inputs_ids, dtype=torch.int32, \n",
    "                                                    device=\"cpu\")\n",
    "\n",
    "        offset_mappings = [[(input_token.idx, input_token.idx + len(input_token))\n",
    "                            for input_token in input_tokens] +\n",
    "                           ((tokens_max_length - len(input_tokens)) * [(0, 0)])\n",
    "                           for input_tokens in inputs_tokens]\n",
    "        tokenize_result[\"offset_mapping\"] = torch.tensor(offset_mappings, dtype=torch.int32, \n",
    "                                                         device=\"cpu\")\n",
    "\n",
    "        if self.tokenize_char:\n",
    "            inputs_tokens_char_ids = (\n",
    "                [[[self.char_vocab_id_mapping.get(char, self.char_vocab_len) \n",
    "                   for char in list(input_low_pad_token)] +\n",
    "                   ((token_max_length - len(input_low_pad_token)) *\n",
    "                    [self.char_vocab_len + 1])\n",
    "                  for input_low_pad_token in input_low_pad_tokens]\n",
    "                 for input_low_pad_tokens in inputs_low_pad_tokens]\n",
    "            )\n",
    "            tokenize_result[\"inputs_char_ids\"] = torch.tensor(inputs_tokens_char_ids, \n",
    "                                                              dtype=torch.int32, \n",
    "                                                              device=\"cpu\")\n",
    "        \n",
    "        return tokenize_result\n",
    "\n",
    "    def tokenize_qa_data(self, contexts, queries, answers,\n",
    "                         context_max_length, query_max_length, token_max_length):\n",
    "        contexts_tokens = self(contexts, context_max_length, token_max_length)\n",
    "        queries_tokens = self(queries, query_max_length, token_max_length)\n",
    "        answers_chars_range = [(answer[\"answer_start\"], \n",
    "                                answer[\"answer_start\"] + len(answer[\"text\"]))\n",
    "                               for answer in answers]\n",
    "        answers_tokens_range = char_ranges_to_token_ranges(answers_chars_range, \n",
    "                                                           contexts_tokens[\"offset_mapping\"],\n",
    "                                                           context_max_length)\n",
    "        return contexts_tokens, queries_tokens, answers_tokens_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_tokenizer = GLoVETokenizer(tokenize_char=True)\n",
    "\n",
    "context_max_length = 300\n",
    "query_max_length = 40\n",
    "token_max_length = 40\n",
    "\n",
    "train_tokens_cqa = glove_tokenizer.tokenize_qa_data(\n",
    "    *(train_tcqa[1:]), context_max_length, query_max_length, token_max_length\n",
    ")\n",
    "dev_tokens_cqa = glove_tokenizer.tokenize_qa_data(\n",
    "    *(dev_tcqa[1:]), context_max_length, query_max_length, token_max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, contexts_tokens, queries_tokens, answers_tokens_range,\n",
    "                 device = \"cpu\"):\n",
    "\n",
    "        to_remove = ~(contexts_tokens[\"truncated\"] | queries_tokens[\"truncated\"])\n",
    "        self.contexts_tokens = {k:v[to_remove] for k,v in contexts_tokens.items()}\n",
    "        self.queries_tokens = {k:v[to_remove] for k,v in queries_tokens.items()}\n",
    "        self.answers_tokens_range = answers_tokens_range[to_remove]\n",
    "        self.device = device\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\"contexts_tokens\": {k:v[index].to(self.device)\n",
    "                                    for k,v in self.contexts_tokens.items()},\n",
    "                \"queries_tokens\": {k:v[index].to(self.device)\n",
    "                                    for k,v in self.queries_tokens.items()},\n",
    "                \"answers_tokens_range\": self.answers_tokens_range[index].to(self.device)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.answers_tokens_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLoVEWordEncoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, corpus=\"6B\", embedding_size=50, char_encoder=None):\n",
    "        \n",
    "        super(GLoVEWordEncoder, self).__init__()\n",
    "        with open(f\"../word_encoders_configs/glove.{corpus}.{embedding_size}d.txt\", \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "        lines = [line.replace(\"\\n\", \"\").split(\" \") for line in lines]\n",
    "        word_embeddings = torch.zeros((len(lines) + 2, embedding_size))\n",
    "        word_embeddings[:len(lines), :] = torch.tensor(\n",
    "            [[float(embedding) for embedding in line[1:]] for line in lines]\n",
    "        )\n",
    "        self.word_embeddings = torch.nn.Embedding.from_pretrained(word_embeddings,\n",
    "                                                                  padding_idx=len(lines) + 1)\n",
    "        self.char_encoder = char_encoder\n",
    "\n",
    "    def forward(self, tokens):\n",
    "\n",
    "        word_embeddings = self.word_embeddings(tokens[\"input_ids\"])\n",
    "        return word_embeddings\n",
    "\n",
    "    def output_shape(self):\n",
    "        output_shape = self.word_embeddings.weight.shape[1]\n",
    "        if self.char_encoder is not None:\n",
    "            output_shape = output_shape + self.char_encoder.output_shape()\n",
    "        return self.word_embeddings.weight.shape[1]\n",
    "\n",
    "\n",
    "class CharCNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 char_vocab_len,\n",
    "                 char_embedding_size, \n",
    "                 outs_channels,\n",
    "                 kernel_sizes):\n",
    "\n",
    "        super(CharCNN, self).__init__()\n",
    "        self.char_encoder = torch.nn.Embedding(char_vocab_len + 3, \n",
    "                                               char_embedding_size, \n",
    "                                               char_vocab_len + 2)\n",
    "        self.in_channels = char_embedding_size\n",
    "        self.outs_channels = outs_channels\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.conv1d_list = list()\n",
    "        for i, kernel_size in enumerate(kernel_sizes):\n",
    "            conv1d = torch.nn.Conv1d(char_embedding_size, outs_channels[i], kernel_size)\n",
    "            setattr(self, f\"conv{i+1}\", conv1d)\n",
    "            self.conv1d_list.append(conv1d)\n",
    "\n",
    "    def forward(self, char_tokens):\n",
    "\n",
    "        char_embeddings = self.char_encoder(char_tokens)\n",
    "\n",
    "        batch_size = char_embeddings.shape[0]\n",
    "        token_seq_size = char_embeddings.shape[1]\n",
    "        token_char_seq_size = char_embeddings.shape[2]\n",
    "        features_size = char_embeddings.shape[3]\n",
    "        \n",
    "        rav_char_embeddings = (char_embeddings.transpose(3, 2)\n",
    "                                              .reshape(batch_size * token_seq_size,\n",
    "                                                       features_size,\n",
    "                                                       token_char_seq_size))\n",
    "        rav_char_embeddings = torch.cat(\n",
    "            [torch.nn.Tanh()(conv1d(rav_char_embeddings)).max(dim=2)[0]\n",
    "             for conv1d in self.conv1d_list],\n",
    "             dim=1\n",
    "        ).reshape(batch_size, token_seq_size, -1)\n",
    "        return rav_char_embeddings\n",
    "\n",
    "    def output_shape(self):\n",
    "        return sum([conv1d.out_channels for conv1d in self.conv1d_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_cnn = CharCNN(1371, 16, [100], [5])\n",
    "\n",
    "glove_word_encoder = GLoVEWordEncoder(embedding_size=200, char_encoder=char_cnn)\n",
    "contextual_embedding_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidaf = model.BiDAF(glove_word_encoder, contextual_embedding_size, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1345 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/pbeuran/personal_repositories/deepqa/notebooks/SQuAD1.1 - Model.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pbeuran/personal_repositories/deepqa/notebooks/SQuAD1.1%20-%20Model.ipynb#ch0000034vscode-remote?line=4'>5</a>\u001b[0m dev_loader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(dev_dataset, \u001b[39m64\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pbeuran/personal_repositories/deepqa/notebooks/SQuAD1.1%20-%20Model.ipynb#ch0000034vscode-remote?line=6'>7</a>\u001b[0m bidaf_trainer \u001b[39m=\u001b[39m wrapper\u001b[39m.\u001b[39mQATrainWrapper(bidaf)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pbeuran/personal_repositories/deepqa/notebooks/SQuAD1.1%20-%20Model.ipynb#ch0000034vscode-remote?line=7'>8</a>\u001b[0m bidaf_trainer\u001b[39m.\u001b[39;49mtrain(train_loader, \n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pbeuran/personal_repositories/deepqa/notebooks/SQuAD1.1%20-%20Model.ipynb#ch0000034vscode-remote?line=8'>9</a>\u001b[0m                     epochs\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m, \n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pbeuran/personal_repositories/deepqa/notebooks/SQuAD1.1%20-%20Model.ipynb#ch0000034vscode-remote?line=9'>10</a>\u001b[0m                     loss\u001b[39m=\u001b[39;49mloss\u001b[39m.\u001b[39;49mbi_cross_entropy,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pbeuran/personal_repositories/deepqa/notebooks/SQuAD1.1%20-%20Model.ipynb#ch0000034vscode-remote?line=10'>11</a>\u001b[0m                     val_data_iter\u001b[39m=\u001b[39;49mdev_loader)\n",
      "File \u001b[0;32m~/personal_repositories/deepqa/wrapper.py:27\u001b[0m, in \u001b[0;36mQATrainWrapper.train\u001b[0;34m(self, train_data_iter, epochs, loss, metrics, val_data_iter)\u001b[0m\n\u001b[1;32m     25\u001b[0m epoch_total_metrics \u001b[39m=\u001b[39m {k:\u001b[39m0\u001b[39m \u001b[39mfor\u001b[39;00m k,v \u001b[39min\u001b[39;00m metrics\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m     26\u001b[0m data_iter_tqdm \u001b[39m=\u001b[39m tqdm(train_data_iter)\n\u001b[0;32m---> 27\u001b[0m \u001b[39mfor\u001b[39;00m i, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(data_iter_tqdm):\n\u001b[1;32m     28\u001b[0m     contexts_tokens, queries_tokens, answers_tokens_range \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(batch\u001b[39m.\u001b[39mvalues())\n\u001b[1;32m     29\u001b[0m     answers_probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel([contexts_tokens, queries_tokens], \u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    531\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    569\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    572\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32m/home/pbeuran/personal_repositories/deepqa/notebooks/SQuAD1.1 - Model.ipynb Cell 17\u001b[0m in \u001b[0;36mQADataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pbeuran/personal_repositories/deepqa/notebooks/SQuAD1.1%20-%20Model.ipynb#ch0000034vscode-remote?line=11'>12</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pbeuran/personal_repositories/deepqa/notebooks/SQuAD1.1%20-%20Model.ipynb#ch0000034vscode-remote?line=12'>13</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mcontexts_tokens\u001b[39m\u001b[39m\"\u001b[39m: {k:v[index]\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pbeuran/personal_repositories/deepqa/notebooks/SQuAD1.1%20-%20Model.ipynb#ch0000034vscode-remote?line=13'>14</a>\u001b[0m                                 \u001b[39mfor\u001b[39;00m k,v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontexts_tokens\u001b[39m.\u001b[39mitems()},\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pbeuran/personal_repositories/deepqa/notebooks/SQuAD1.1%20-%20Model.ipynb#ch0000034vscode-remote?line=14'>15</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mqueries_tokens\u001b[39m\u001b[39m\"\u001b[39m: {k:v[index]\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pbeuran/personal_repositories/deepqa/notebooks/SQuAD1.1%20-%20Model.ipynb#ch0000034vscode-remote?line=15'>16</a>\u001b[0m                                 \u001b[39mfor\u001b[39;00m k,v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqueries_tokens\u001b[39m.\u001b[39mitems()},\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pbeuran/personal_repositories/deepqa/notebooks/SQuAD1.1%20-%20Model.ipynb#ch0000034vscode-remote?line=16'>17</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39manswers_tokens_range\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39manswers_tokens_range[index]\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)}\n",
      "\u001b[1;32m/home/pbeuran/personal_repositories/deepqa/notebooks/SQuAD1.1 - Model.ipynb Cell 17\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pbeuran/personal_repositories/deepqa/notebooks/SQuAD1.1%20-%20Model.ipynb#ch0000034vscode-remote?line=11'>12</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pbeuran/personal_repositories/deepqa/notebooks/SQuAD1.1%20-%20Model.ipynb#ch0000034vscode-remote?line=12'>13</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mcontexts_tokens\u001b[39m\u001b[39m\"\u001b[39m: {k:v[index]\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pbeuran/personal_repositories/deepqa/notebooks/SQuAD1.1%20-%20Model.ipynb#ch0000034vscode-remote?line=13'>14</a>\u001b[0m                                 \u001b[39mfor\u001b[39;00m k,v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontexts_tokens\u001b[39m.\u001b[39mitems()},\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pbeuran/personal_repositories/deepqa/notebooks/SQuAD1.1%20-%20Model.ipynb#ch0000034vscode-remote?line=14'>15</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mqueries_tokens\u001b[39m\u001b[39m\"\u001b[39m: {k:v[index]\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pbeuran/personal_repositories/deepqa/notebooks/SQuAD1.1%20-%20Model.ipynb#ch0000034vscode-remote?line=15'>16</a>\u001b[0m                                 \u001b[39mfor\u001b[39;00m k,v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqueries_tokens\u001b[39m.\u001b[39mitems()},\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pbeuran/personal_repositories/deepqa/notebooks/SQuAD1.1%20-%20Model.ipynb#ch0000034vscode-remote?line=16'>17</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39manswers_tokens_range\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39manswers_tokens_range[index]\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)}\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:216\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    213\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    214\u001b[0m \u001b[39m# This function throws if there's a driver initialization error, no GPUs\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[39m# are found or any other error occurs\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_init()\n\u001b[1;32m    217\u001b[0m \u001b[39m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[39m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[39m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    220\u001b[0m _tls\u001b[39m.\u001b[39mis_initializing \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "train_dataset = QADataset(*train_tokens_cqa, \"cuda\")\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, 64)\n",
    "\n",
    "dev_dataset = QADataset(*dev_tokens_cqa, \"cuda\")\n",
    "dev_loader = torch.utils.data.DataLoader(dev_dataset, 64)\n",
    "\n",
    "bidaf_trainer = wrapper.QATrainWrapper(bidaf)\n",
    "bidaf_trainer.train(train_loader, \n",
    "                    epochs=16, \n",
    "                    loss=loss.bi_cross_entropy,\n",
    "                    val_data_iter=dev_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
