{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==1.11 in /home/pbeuran/.local/lib/python3.8/site-packages (1.11.0)\n",
      "Requirement already satisfied: transformers in /home/pbeuran/.local/lib/python3.8/site-packages (4.19.2)\n",
      "Collecting stanza\n",
      "  Downloading stanza-1.4.0-py3-none-any.whl (574 kB)\n",
      "\u001b[K     |████████████████████████████████| 574 kB 22.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /home/pbeuran/.local/lib/python3.8/site-packages (from torch==1.11) (4.2.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/pbeuran/.local/lib/python3.8/site-packages (from transformers) (2022.4.24)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/pbeuran/.local/lib/python3.8/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/pbeuran/.local/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/pbeuran/.local/lib/python3.8/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/pbeuran/.local/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: filelock in /home/pbeuran/.local/lib/python3.8/site-packages (from transformers) (3.7.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/pbeuran/.local/lib/python3.8/site-packages (from transformers) (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/pbeuran/.local/lib/python3.8/site-packages (from transformers) (1.22.1)\n",
      "Requirement already satisfied: requests in /home/pbeuran/.local/lib/python3.8/site-packages (from transformers) (2.27.1)\n",
      "Collecting emoji\n",
      "  Downloading emoji-2.0.0.tar.gz (197 kB)\n",
      "\u001b[K     |████████████████████████████████| 197 kB 23.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf in /home/pbeuran/.local/lib/python3.8/site-packages (from stanza) (3.20.1)\n",
      "Requirement already satisfied: six in /home/pbeuran/.local/lib/python3.8/site-packages (from stanza) (1.16.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/pbeuran/.local/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/pbeuran/.local/lib/python3.8/site-packages (from requests->transformers) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /home/pbeuran/.local/lib/python3.8/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /home/pbeuran/.local/lib/python3.8/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/pbeuran/.local/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "Building wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for emoji: filename=emoji-2.0.0-py3-none-any.whl size=193002 sha256=f2ba39c5bfc065aae279bed1af2dbc61228ec33cdc462937c87bff48a815fdbb\n",
      "  Stored in directory: /home/pbeuran/.cache/pip/wheels/23/a5/a8/e74bad1ceced228b6ae94dcbacc5c67df6486fd1620714e7d1\n",
      "Successfully built emoji\n",
      "Installing collected packages: emoji, stanza\n",
      "Successfully installed emoji-2.0.0 stanza-1.4.0\n"
     ]
    }
   ],
   "source": [
    "#####################################################################\n",
    "# Mandatory : Run this cell and restart the notebook kernel right after\n",
    "#####################################################################\n",
    "!pip install torch==1.11 transformers stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/content'\n",
      "/home/pbeuran/personal_repositories/deepqa/notebooks\n"
     ]
    }
   ],
   "source": [
    "%cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#####################################################################\n",
    "# Only use on Googgle Colab, uncomment if necessary\n",
    "#####################################################################\n",
    "# Clone the repo content into\n",
    "cd /content\n",
    "rm -rf deepqa\n",
    "git clone -b model https://github.com/PaulBeuran/deepqa.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#####################################################################\n",
    "# Only use on Googgle Colab, uncomment if necessary\n",
    "#####################################################################\n",
    "# Clone the repo content into\n",
    "cd /content\n",
    "rm -rf deepqa\n",
    "git clone -b model https://github.com/PaulBeuran/deepqa.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'deepqa/notebooks/'\n",
      "/home/pbeuran/personal_repositories/deepqa/notebooks\n"
     ]
    }
   ],
   "source": [
    "#####################################################################\n",
    "# Only use on Googgle Colab, uncomment if necessary\n",
    "#####################################################################\n",
    "%cd deepqa/notebooks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  822M  100  822M    0     0  4124k      0  0:03:24  0:03:24 --:--:-- 4146k\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  glove.6B.zip\n",
      "  inflating: word_encoders_configs/glove.6B.50d.txt  \n",
      "  inflating: word_encoders_configs/glove.6B.100d.txt  \n",
      "  inflating: word_encoders_configs/glove.6B.200d.txt  \n",
      "  inflating: word_encoders_configs/glove.6B.300d.txt  \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ..\n",
    "curl -O https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
    "unzip -o glove.6B.zip -d word_encoders_configs\n",
    "rm glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-30 00:36:05.538503: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-30 00:36:05.538577: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Dowload the SQuAD1.1 data\n",
    "curl -O https://data.deepai.org/squad1.1.zip\n",
    "unzip -o squad1.1.zip -d data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-06 14:18:17.022434: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-08-06 14:18:17.022510: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-08-06 14:18:19.649332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-08-06 14:18:19.649555: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-08-06 14:18:19.649653: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-08-06 14:18:19.649746: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-08-06 14:18:19.649838: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-08-06 14:18:19.649930: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-08-06 14:18:19.650047: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-08-06 14:18:19.650142: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-08-06 14:18:19.650246: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-08-06 14:18:19.650261: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "TORCH_CUDA_IS_AVAILABLE = torch.cuda.is_available()\n",
    "\n",
    "if TORCH_CUDA_IS_AVAILABLE:\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "deepqa_lib_path = str(pathlib.Path(os.getcwd()).parent.parent.absolute())\n",
    "sys.path.insert(0, deepqa_lib_path)\n",
    "\n",
    "from deepqa import utils, preprocessing, tokenizer, module, model, wrapper, loss, metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read train and dev data\n",
    "with open(\"data/train-v1.1.json\", \"rb\") as j:\n",
    "    train_data = json.load(j)[\"data\"]\n",
    "with open(\"data/dev-v1.1.json\", \"rb\") as j:\n",
    "    dev_data = json.load(j)[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tcqa = preprocessing.tabularize_squad11_data(train_data)\n",
    "dev_tcqa = preprocessing.tabularize_squad11_data(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_tokenizer = tokenizer.GLoVETokenizer(tokenize_char=True)\n",
    "\n",
    "context_max_length = 300\n",
    "query_max_length = 40\n",
    "token_max_length = 40\n",
    "\n",
    "train_tokens_cqa = glove_tokenizer.tokenize_qa_data(\n",
    "    *(train_tcqa[1:]), context_max_length, query_max_length, token_max_length\n",
    ")\n",
    "dev_tokens_cqa = glove_tokenizer.tokenize_qa_data(\n",
    "    *(dev_tcqa[1:]), context_max_length, query_max_length, token_max_length\n",
    ")\n",
    "\n",
    "train_dataset = utils.QADataset(*train_tokens_cqa, \"cuda\" if TORCH_CUDA_IS_AVAILABLE\n",
    "                                                          else \"cpu\")\n",
    "dev_dataset = utils.QADataset(*dev_tokens_cqa, \"cuda\" if TORCH_CUDA_IS_AVAILABLE\n",
    "                                                      else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_cnn = module.CharCNN(1371, 16, [100], [5])\n",
    "\n",
    "glove_word_encoder = module.GLoVEWordEncoder(embedding_size=200, char_encoder=char_cnn)\n",
    "contextual_embedding_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidaf = model.BiDAF(glove_word_encoder, contextual_embedding_size, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, 64)\n",
    "dev_loader = torch.utils.data.DataLoader(dev_dataset, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm \n",
    "from deepqa.metrics import exact_match, overlap_f1_score\n",
    "from deepqa.loss import bi_cross_entropy\n",
    "\n",
    "class QATrainWrapper(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, model, \n",
    "                 context_max_length=512, query_max_length=64, \n",
    "                 verbose=True):\n",
    "\n",
    "        super(QATrainWrapper, self).__init__()\n",
    "        self.model = model\n",
    "        self.context_max_length = context_max_length\n",
    "        self.query_max_length = query_max_length\n",
    "        self.optimizer = torch.optim.Adam(model.parameters())\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def train(self, train_data_iter, epochs, \n",
    "              loss=bi_cross_entropy, \n",
    "              metrics={\"EM\": exact_match, \"F1\": overlap_f1_score},\n",
    "              val_data_iter=None):\n",
    "\n",
    "        train_dataset_len = len(train_data_iter.dataset)\n",
    "        train_batch_size = train_data_iter.batch_size\n",
    "        train_loader_n_iter = len(train_data_iter)\n",
    "        train_loss_by_epochs = np.zeros((epochs * train_dataset_len, 2))\n",
    "        train_loss_by_epochs[:, 0] = np.repeat(np.arange(epochs), train_dataset_len)\n",
    "        train_metrics_by_epochs = np.zeros((epochs * train_dataset_len, len(metrics) + 1))\n",
    "        train_metrics_by_epochs[:, 0] = np.repeat(np.arange(epochs), train_dataset_len)\n",
    "        if val_data_iter is not None:\n",
    "            val_dataset_len = len(val_data_iter.dataset)\n",
    "            val_batch_size = val_data_iter.batch_size\n",
    "            val_loader_n_iter = len(val_data_iter)\n",
    "            val_loss_by_epochs = np.zeros((epochs * val_dataset_len, 2))\n",
    "            val_loss_by_epochs[:, 0] = np.repeat(np.arange(epochs), val_dataset_len)\n",
    "            val_metrics_by_epochs = np.zeros((epochs * val_dataset_len, len(metrics) + 1))\n",
    "            val_metrics_by_epochs[:, 0] = np.repeat(np.arange(epochs), val_dataset_len)\n",
    "            \n",
    "        for epoch in range(epochs):\n",
    "            epoch_total_loss = 0\n",
    "            epoch_total_metrics = {k:0 for k in metrics}\n",
    "            data_iter_tqdm = tqdm(train_data_iter) if self.verbose else train_data_iter\n",
    "            for i, batch in enumerate(data_iter_tqdm):\n",
    "                contexts_tokens, queries_tokens, answers_tokens_range = tuple(batch.values())\n",
    "                answers_probs = self.model([contexts_tokens, queries_tokens], True)\n",
    "                answer_preds = answers_probs.argmax(dim=1)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss_batch = loss(answers_probs, answers_tokens_range)\n",
    "                loss_step = loss_batch.mean()\n",
    "                loss_step.backward()\n",
    "                self.optimizer.step()\n",
    "                metrics_batch_dict = {k : metric(answer_preds, answers_tokens_range)\n",
    "                                      for k,metric in metrics.items()}\n",
    "                metrics_batch = torch.cat(list(metrics_batch_dict.values()), axis=1)            \n",
    "                min_slice = (train_loader_n_iter * epoch) + (train_batch_size * i)\n",
    "                max_slice = ((train_loader_n_iter * epoch) + (train_batch_size * i) +\n",
    "                             train_batch_size if i+1 != train_loader_n_iter \n",
    "                                              else train_dataset_len % train_batch_size)\n",
    "                batch_slice = slice(min_slice, max_slice)\n",
    "                train_loss_by_epochs[batch_slice, 1] = loss_batch.to(\"cpu\").numpy()\n",
    "                train_metrics_by_epochs[batch_slice, 1:] = metrics_batch.to(\"cpu\").numpy()\n",
    "                epoch_total_loss += loss_step.to(\"cpu\").item()\n",
    "                epoch_total_metrics = {k : epoch_total_metrics[k] +\n",
    "                                           metrics_batch_dict[k].mean().to(\"cpu\").item()\n",
    "                                       for k in metrics}\n",
    "                if self.verbose:\n",
    "                    data_iter_tqdm.set_description(f\"\"\"Epoch {epoch+1}/{epochs} - Train - Loss: {\n",
    "                        round(epoch_total_loss/(i+1), 3)}, Metrics: {\n",
    "                            {id:round(etm/(i+1), 3) for id, etm in epoch_total_metrics.items()}}\"\"\")\n",
    "\n",
    "            if val_data_iter is not None:\n",
    "                epoch_total_loss = 0\n",
    "                epoch_total_metrics = {k:0 for k in metrics}\n",
    "                data_iter_tqdm = tqdm(val_data_iter) if self.verbose else val_data_iter\n",
    "                for i, batch in enumerate(data_iter_tqdm):\n",
    "                    with torch.no_grad():\n",
    "                        contexts_tokens, queries_tokens, answers_tokens_range = tuple(batch.values())\n",
    "                        answers_probs = self.model([contexts_tokens, queries_tokens])\n",
    "                        answer_preds = answers_probs.argmax(dim=1)\n",
    "                        loss_batch = loss(answers_probs, answers_tokens_range)\n",
    "                        loss_step = loss_batch.mean()\n",
    "                        metrics_batch_dict = {k : metric(answer_preds, answers_tokens_range)\n",
    "                                            for k,metric in metrics.items()}\n",
    "                        metrics_batch = torch.cat(list(metrics_batch_dict.values()), axis=1)            \n",
    "                        min_slice = (val_loader_n_iter * epoch) + (val_batch_size * i)\n",
    "                        max_slice = ((val_loader_n_iter * epoch) + (val_batch_size * i) +\n",
    "                                    val_batch_size if i+1 != val_loader_n_iter \n",
    "                                                    else val_dataset_len % val_batch_size)\n",
    "                        batch_slice = slice(min_slice, max_slice)\n",
    "                        val_loss_by_epochs[batch_slice, 1] = loss_batch.to(\"cpu\").numpy()\n",
    "                        val_metrics_by_epochs[batch_slice, 1:] = metrics_batch.to(\"cpu\").numpy()\n",
    "                        epoch_total_loss += loss_step.to(\"cpu\").item()\n",
    "                        epoch_total_metrics = {k : epoch_total_metrics[k] +\n",
    "                                                metrics_batch_dict[k].mean().to(\"cpu\").item()\n",
    "                                            for k in metrics}\n",
    "                        if self.verbose:\n",
    "                            data_iter_tqdm.set_description(f\"\"\"Epoch {epoch+1}/{epochs} - Val - Loss: {\n",
    "                                round(epoch_total_loss/(i+1), 3)}, Metrics: {\n",
    "                                    {id:round(etm/(i+1), 3) for id, etm in epoch_total_metrics.items()}}\"\"\")\n",
    "\n",
    "        to_return = [train_loss_by_epochs, train_metrics_by_epochs]\n",
    "        if val_data_iter is not None:\n",
    "            to_return.extend([val_loss_by_epochs, val_metrics_by_epochs])\n",
    "        return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/pbeuran/personal_repositories/deepqa/notebooks/SQuAD1.1 - Model.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pbeuran/personal_repositories/deepqa/notebooks/SQuAD1.1%20-%20Model.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m bidaf_trainer \u001b[39m=\u001b[39m wrapper\u001b[39m.\u001b[39mQATrainWrapper(bidaf)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pbeuran/personal_repositories/deepqa/notebooks/SQuAD1.1%20-%20Model.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m bidaf_trainer\u001b[39m.\u001b[39;49mtrain(train_loader, \n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pbeuran/personal_repositories/deepqa/notebooks/SQuAD1.1%20-%20Model.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m                     epochs\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m, \n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pbeuran/personal_repositories/deepqa/notebooks/SQuAD1.1%20-%20Model.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m                     loss\u001b[39m=\u001b[39;49mloss\u001b[39m.\u001b[39;49mbi_cross_entropy,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pbeuran/personal_repositories/deepqa/notebooks/SQuAD1.1%20-%20Model.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m                     val_data_iter\u001b[39m=\u001b[39;49mdev_loader)\n",
      "File \u001b[0;32m~/personal_repositories/deepqa/wrapper.py:29\u001b[0m, in \u001b[0;36mQATrainWrapper.train\u001b[0;34m(self, train_data_iter, epochs, loss, metrics, val_data_iter)\u001b[0m\n\u001b[1;32m     27\u001b[0m train_loader_n_iter \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(train_data_iter)\n\u001b[1;32m     28\u001b[0m train_loss_by_epochs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray((epochs \u001b[39m*\u001b[39m train_dataset_len, \u001b[39m2\u001b[39m))\n\u001b[0;32m---> 29\u001b[0m train_loss_by_epochs[:, \u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrepeat(np\u001b[39m.\u001b[39marange(epochs), train_dataset_len)\n\u001b[1;32m     30\u001b[0m train_metrics_by_epochs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray((epochs \u001b[39m*\u001b[39m train_dataset_len, \u001b[39mlen\u001b[39m(metrics) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))\n\u001b[1;32m     31\u001b[0m train_metrics_by_epochs[:, \u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrepeat(np\u001b[39m.\u001b[39marange(epochs), train_dataset_len)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "bidaf_trainer = wrapper.QATrainWrapper(bidaf)\n",
    "bidaf_trainer.train(train_loader, \n",
    "                    epochs=16, \n",
    "                    loss=loss.bi_cross_entropy,\n",
    "                    val_data_iter=dev_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.5"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(10 + 5 + 7 + 8)/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [10, 5, 7, 8]\n",
    "acc = 0\n",
    "for i in range(4):\n",
    "    acc = acc "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
